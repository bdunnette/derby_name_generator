{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "derby_names.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdunnette/derby_name_generator/blob/notebook/derby_names.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMO1Y1Qj5LWt",
        "colab_type": "text"
      },
      "source": [
        "Based on Max Woolf's notebook: https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing\n",
        "\n",
        "Inspired by Janelle Shane's blog post: http://aiweirdness.com/post/174466734677/neural-network-generated-roller-derby-names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5sZ7k49z9FR",
        "colab_type": "code",
        "outputId": "1601ab4b-aa11-4e62-ab4b-7a9660804142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!pip install tensorflow textgenrnn tensorflowjs\n",
        "from google.colab import files\n",
        "from textgenrnn import textgenrnn\n",
        "training_file = \"derby_names.txt\"\n",
        "model_name = 'derbynames' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.14.0rc1)\n",
            "Requirement already satisfied: textgenrnn in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Collecting tensorflowjs\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ea/5ef7904c720f22c78e9efbd7aa7473ac08c21f09b495961c93ca374e3423/tensorflowjs-1.2.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0rc1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.21.2)\n",
            "Requirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.2.4)\n",
            "Collecting tensorflow-hub==0.3.0 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/f0/3a3ced04c8359e562f1b91918d9bde797c8a916fcfeddc8dc5d673d1be20/tensorflow_hub-0.3.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->textgenrnn) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->textgenrnn) (0.13.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "\u001b[31mERROR: tensorflowjs 1.2.2.1 has requirement six==1.11.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflowjs 1.2.2.1 has requirement tensorflow==1.14.0, but you'll have tensorflow 1.14.0rc1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: tensorflow-hub 0.4.0\n",
            "    Uninstalling tensorflow-hub-0.4.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.4.0\n",
            "Successfully installed tensorflow-hub-0.3.0 tensorflowjs-1.2.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0o8T_WdCy7X",
        "colab_type": "code",
        "outputId": "10edde1f-2588-4109-c843-dfadea137db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "name_set = set()\n",
        "session = requests.Session()\n",
        "\n",
        "def clean_name(text):\n",
        "#   strip whitespace and remove parentheticals\n",
        "  return re.sub(r\" ?\\([^)]+\\)\", \"\", text.strip())\n",
        "  \n",
        "url1 = \"https://www.twoevils.org/rollergirls/\"\n",
        "print(\"Getting names from {}\".format(url1))\n",
        "r1 = session.get(url1)\n",
        "d1 = r1.text\n",
        "soup1 = BeautifulSoup(d1, \"lxml\")\n",
        "rows1 = soup1.find_all('tr', {'class':['trc1', 'trc2']})\n",
        "\n",
        "for idx, row in enumerate(rows1):\n",
        "    td = row.find('td')\n",
        "    name = clean_name(td.get_text())\n",
        "    name_set.add(name)\n",
        "    \n",
        "print(\"Downloaded {} names\".format(len(name_set)))\n",
        "\n",
        "url2 = \"http://www.derbyrollcall.com/everyone\"\n",
        "print(\"Getting names from {}\".format(url2))\n",
        "r2 = session.get(url2)\n",
        "d2 = r2.text\n",
        "soup2 = BeautifulSoup(d2, \"lxml\")\n",
        "rows2 = soup2.find_all('td', {'class':'name'})\n",
        "\n",
        "for idx, td in enumerate(rows2):\n",
        "    name = clean_name(td.get_text())\n",
        "    name_set.add(name)\n",
        "\n",
        "print(\"Downloaded {} names\".format(len(name_set)))\n",
        "    \n",
        "initial_letters = string.ascii_uppercase\n",
        "# Loop through initial letters (A-Z)\n",
        "for letter in initial_letters:\n",
        "  url3 = \"https://rollerderbyroster.com/view-names/?ini={}\".format(letter)\n",
        "  print(\"Getting names from {}\".format(url3))\n",
        "  r3 = session.get(url3)\n",
        "  d3 = r3.text\n",
        "  soup3 = BeautifulSoup(d3, \"lxml\")\n",
        "  \n",
        "  rows3 = soup3.find_all('ul')\n",
        "  # Use only last unordered list - this is where names are!\n",
        "  for idx, li in enumerate(rows3[-1]):\n",
        "    # Name should be the text of the link within the list item\n",
        "    name = clean_name(li.find('a').get_text())\n",
        "#     print(name)\n",
        "    name_set.add(name)\n",
        "  print(\"Downloaded {} names\".format(len(name_set)))    \n",
        "    \n",
        "with open(training_file,\"w\") as names_file:\n",
        "    name_list = list(name_set)\n",
        "    print(\"Writing {} names to {}\".format(len(name_list),training_file))\n",
        "#     name_list.sort()\n",
        "    random.shuffle(name_list)\n",
        "    names_file.writelines(\"%s\\n\" % n for n in name_list)\n",
        "#     files.download(training_file)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting names from https://www.twoevils.org/rollergirls/\n",
            "Downloaded 40509 names\n",
            "Getting names from http://www.derbyrollcall.com/everyone\n",
            "Downloaded 69258 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=A\n",
            "Downloaded 69364 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=B\n",
            "Downloaded 69601 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=C\n",
            "Downloaded 69756 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=D\n",
            "Downloaded 69972 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=E\n",
            "Downloaded 70024 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=F\n",
            "Downloaded 70124 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=G\n",
            "Downloaded 70200 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=H\n",
            "Downloaded 70331 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=I\n",
            "Downloaded 70382 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=J\n",
            "Downloaded 70497 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=K\n",
            "Downloaded 70615 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=L\n",
            "Downloaded 70836 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=M\n",
            "Downloaded 71025 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=N\n",
            "Downloaded 71076 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=O\n",
            "Downloaded 71120 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=P\n",
            "Downloaded 71236 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=Q\n",
            "Downloaded 71247 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=R\n",
            "Downloaded 71400 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=S\n",
            "Downloaded 71756 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=T\n",
            "Downloaded 71887 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=U\n",
            "Downloaded 71892 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=V\n",
            "Downloaded 71951 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=W\n",
            "Downloaded 72011 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=X\n",
            "Downloaded 72014 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=Y\n",
            "Downloaded 72020 names\n",
            "Getting names from https://rollerderbyroster.com/view-names/?ini=Z\n",
            "Downloaded 72028 names\n",
            "Writing 72028 names to derby_names.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hivPLDl5C3zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cfg = {\n",
        "    'word_level': False,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
        "    'rnn_size': 128,   # number of LSTM cells of each layer (128/256 recommended)\n",
        "    'rnn_layers': 3,   # number of LSTM layers (>=2 recommended)\n",
        "    'rnn_bidirectional': True,   # consider text both forwards and backward, can give a training boost\n",
        "    'max_length': 20,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
        "    'max_words': 10000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,   # set to True if each text has its own line in the source file\n",
        "    'num_epochs': 20,   # set higher to train the model for longer\n",
        "    'gen_epochs': 5,   # generates sample text from model after given number of epochs\n",
        "    'train_size': 1.0,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
        "    'dropout': 0.2,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
        "    'validation': False,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
        "    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfg9-JRr09Z9",
        "colab_type": "code",
        "outputId": "caf13160-bb8a-4ecc-b27e-6aa4a7adbe39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=training_file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=1024,\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=100,\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 3-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 984,669 character sequences.\n",
            "Epoch 1/20\n",
            "961/961 [==============================] - 73s 76ms/step - loss: 2.4687\n",
            "Epoch 2/20\n",
            "961/961 [==============================] - 70s 73ms/step - loss: 2.1084\n",
            "Epoch 3/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.9990\n",
            "Epoch 4/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.9368\n",
            "Epoch 5/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.8913\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            " Mangler\n",
            "Trixie Bomb\n",
            "Silver Slammer\n",
            "Bashing Bang Bang\n",
            "Baby Bang Bang\n",
            "Shank Streak\n",
            "Torture Storm\n",
            "Slam Dead\n",
            "Star Machine\n",
            "Suzie Slammer\n",
            "Cherry Bitch\n",
            "Mary Mary Mayhem\n",
            "Mary Mary Slammer\n",
            "Delia Deadly\n",
            "Sassy Streak\n",
            "Darth Storm\n",
            "Roller Roller\n",
            "The Bomb\n",
            "Danger Stone\n",
            "Bella Bang Bang\n",
            "Slam Danger\n",
            "Dangerous Bang Ba\n",
            "\n",
            "a Bang Bang\n",
            "Bambi Bullet\n",
            "Mary Manson\n",
            "Bang Bang Bang\n",
            "Malice Angel\n",
            "Shell Bang Bang\n",
            "Bang Bang Bang\n",
            "Shank Star\n",
            "Slam Danger\n",
            "Shelly Striper\n",
            "Sally Bang Bang\n",
            "Sally Storm\n",
            "Shank Storm\n",
            "Scarlet Block\n",
            "Sugar Streak\n",
            "Samming Slammer\n",
            "Slam Candy\n",
            "Shell Butter\n",
            "Suzy Stones\n",
            "Tara Bomber\n",
            "Slam Danger\n",
            "The Bang Bang\n",
            "Slam Dang\n",
            "\n",
            "ry Steel\n",
            "Sin Destroyer\n",
            "Slam Steel\n",
            "Tara Bang Bang\n",
            "Mary Star\n",
            "Lil Miss Slammer\n",
            "Bang Bang Bang\n",
            "Buster Bomb\n",
            "Harley Scream\n",
            "Scarlet Striper\n",
            "Big Bomb\n",
            "Judge Slammer\n",
            "Gold Stripes\n",
            "Sassy Slammer\n",
            "Rock N Roll\n",
            "Lil Red Roller\n",
            "Sister Stripes\n",
            "Slam Steel\n",
            "Bella Bomb\n",
            "Shock Machine\n",
            "Silver Slammer\n",
            "Bad Assassin\n",
            "Slammer Fan\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "gel of Blood\n",
            "Crazy Chaser\n",
            "Love Banshee\n",
            "Butter Pain\n",
            "Lady Bruiser\n",
            "Slayer Mangler\n",
            "Hand Block\n",
            "Baby Bitches\n",
            "Pain McCracken\n",
            "Maker Scarlet\n",
            "Bitch Lisa\n",
            "Mad Dead\n",
            "Miss Angel Dark Machine\n",
            "Twisted Seemen\n",
            "Block Ener\n",
            "Blue Blocking\n",
            "Hell O’Sinery\n",
            "Ray Gore\n",
            "Steel Block\n",
            "Shank Tonic\n",
            "Rolling Revolver\n",
            "Magic Deadly Fantast\n",
            "\n",
            "aurus Rex\n",
            "Lady Lola\n",
            "Olivia Steel\n",
            "Peril Doll\n",
            "Collida Bomber\n",
            "Boxx\n",
            "Kalamity Stroy\n",
            "Rolling Zombie\n",
            "Bonarropher Carnage\n",
            "Star Chaos\n",
            "Marlo Mangler\n",
            "Sunshine Betty\n",
            "Barbie Bomber\n",
            "Lola Love\n",
            "The Killah Bomber\n",
            "Mama Pain\n",
            "Ring Storm\n",
            "Raggedy Raven\n",
            "Natalia SlamHer\n",
            "Anne Spanks\n",
            "The Angel Eater\n",
            "Miss Danger\n",
            "Pain De Stroy\n",
            "\n",
            "ruise\n",
            "Mad Star\n",
            "Pat Sparkles\n",
            "Rolling Brawler\n",
            "The Chaos of Candy\n",
            "Scarlet Slam\n",
            "Get Angel\n",
            "Allie A. Kaos\n",
            "Harley Queen\n",
            "Stryk'er Roller\n",
            "Steely Scarlett\n",
            "BOOKIN GRROBLY\n",
            "Trixie Venom\n",
            "Nasty Carnage\n",
            "Natalia Collider\n",
            "Skater Spitfight\n",
            "Vicious Badge\n",
            "Mary Massacre\n",
            "Danger Crack\n",
            "Derby Rains\n",
            "Storm De Stroyer\n",
            "Stella Da\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "mintate\n",
            "Holly Allie\n",
            "Infernadon Hardy\n",
            "Darby Doll\n",
            "Beanica\n",
            "Marla Hurlan\n",
            "Blilie Deadhitt\n",
            "Jen-Ocimust N Tsunaminjam\n",
            "Quadrumbler\n",
            "Lady Pix\n",
            "Mau Brav’Her\n",
            "Jenny Wanda\n",
            "Roller Gina\n",
            "Wicked Maiden\n",
            "Boston C. Blackhear-Em\n",
            "Rocket\n",
            "Emer's Alning\n",
            "Twisted Humockaho\n",
            "Crazy Lickinsaney\n",
            "Bad Kicker\n",
            "Lostan Hellion\n",
            "Rogue N Sla\n",
            "\n",
            "ry Jambitter\n",
            "Battle Russ\n",
            "Latria Leather\n",
            "Foo O’Russ\n",
            "Claire Maims\n",
            "Burgle Harderox\n",
            "Avenrassada\n",
            "Deviant Viking\n",
            "Trallrium\n",
            "Lockin' Gin\n",
            "Pretti Furious\n",
            "Bobbie bloadla\n",
            "Chasee BlockHer\n",
            "Sinnaster Queen\n",
            "Silly Chaos\n",
            "Gan Starpe\n",
            "Mad Fury\n",
            "'Blue Skitzow\n",
            "Rocket Masugals\n",
            "Ella JenDesser\n",
            "PONDY Purche\n",
            "Leadna Schwag\n",
            "ForyD\n",
            "\n",
            "r\n",
            "RIMMAWRECT\n",
            "Grim Ice \"D\n",
            "CHEP ERM’KLIshYa\n",
            "Rapsy Benza\n",
            "Roller Rocknz\n",
            "Bionita Von Bruise\n",
            "Dr. Kenkla\n",
            "Lady Aphron\n",
            "Cheetahaus Savelher\n",
            "Dolly Wrawjack’Em\n",
            "Elle Day\n",
            "AWICHY\n",
            "IvonNestrate\n",
            "Krackin' Daugh\n",
            "Shellam Beezey\n",
            "Corpsemy D-Stitches\n",
            "Fallous Oxidup\n",
            "Pink 'n Pixie\n",
            "Juscual\n",
            "Otch Shelly\n",
            "Zombaro\n",
            "Manic Jacque Bru\n",
            "\n",
            "Epoch 6/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.8553\n",
            "Epoch 7/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.8244\n",
            "Epoch 8/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.7964\n",
            "Epoch 9/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.7708\n",
            "Epoch 10/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.7464\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "herblock Betty\n",
            "Martha Massacre\n",
            "Shell Scream\n",
            "Scarlett O'Hard\n",
            "Shell Storm\n",
            "Shell Storm\n",
            "Slammin' Martini\n",
            "Rollin' Roll\n",
            "Shank Stroyer\n",
            "Star Slammin'\n",
            "Star Star\n",
            "Sin D. Stroya\n",
            "Scarlett O'Hard\n",
            "Skate Trouble\n",
            "Lil' Miss Monkey\n",
            "Shell Storm\n",
            "Smack Ass\n",
            "Sandra Dee Mon\n",
            "Sherry Slammin'\n",
            "Sugar Strike\n",
            "Strawberry Shover\n",
            "Str\n",
            "\n",
            "Crusher\n",
            "Sugar Strangler\n",
            "Rock N Roll\n",
            "Shell Storm\n",
            "Red Red Roll\n",
            "Stella Strike\n",
            "The Block Storm\n",
            "Storm Star\n",
            "Slam Power\n",
            "Cherry Bones\n",
            "Sugar Stripper\n",
            "Sugar Strangler\n",
            "Shank Star\n",
            "Strawberry Slammington\n",
            "Star Stroyer\n",
            "Super Slammin'\n",
            "Shell Star\n",
            "Smash N DeckHER\n",
            "Suzy Starr\n",
            "Mariah Martini\n",
            "Rock N Roll\n",
            "Short Star\n",
            "Miss \n",
            "\n",
            "orm\n",
            "Shell Storm\n",
            "Betty Boom Boom\n",
            "Shell N Strong\n",
            "Strawberry Slammington\n",
            "Sin De Meaner\n",
            "Super Slammer\n",
            "Sherry Strong\n",
            "Shell Strangler\n",
            "Smash N DeckHer\n",
            "Miss Monkey\n",
            "Stella Storm\n",
            "Sparkle Speed\n",
            "Shell Man\n",
            "Strawberry Sparkles\n",
            "Slammin' Manson\n",
            "Stella Strike\n",
            "Red Hot Roll\n",
            "Sassy Slammer\n",
            "Hard Knight\n",
            "The Mad Manson\n",
            "Sho\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "h N Beat'cher\n",
            "Cheese Thunder Bitch\n",
            "Jane Bruiser\n",
            "Trixie Terror\n",
            "Lil Victim\n",
            "Saturday Brawl\n",
            "Finnamonium\n",
            "Sherry Control\n",
            "Sin Assassin\n",
            "Beans N Crusher\n",
            "Hurricane Heidi\n",
            "Sarah Battle\n",
            "Bloody Butcher\n",
            "Ruby Rainbow\n",
            "Charlotta Cannonball\n",
            "Super Slam\n",
            "Triple Beauty\n",
            "Deadly Wings\n",
            "Bitter Booty Bones\n",
            "Serenity Knievel\n",
            "Sire\n",
            "\n",
            "ver Star\n",
            "Crystal Pain\n",
            "Thunder Brown\n",
            "Trixie Tank\n",
            "Angel of Anne\n",
            "Frank Needle\n",
            "Bella Bruiser\n",
            "Dan-n Disorder\n",
            "Rough Red Ref\n",
            "Skates of the Punch\n",
            "Red Hot Roll\n",
            "Sparkle Fury\n",
            "Strawberry Chaos\n",
            "Miss Chaos\n",
            "Miss Wendy Reed Spark\n",
            "Martha Lee Le Bruise\n",
            "Bruiser Bruiser\n",
            "Spider Fire\n",
            "Mad Mary\n",
            "Ginger Bones\n",
            "Kitty Kat\n",
            "Kitty\n",
            "\n",
            "yer\n",
            "Killa Knocks\n",
            "Angel of Steel\n",
            "Rachel Red\n",
            "Strawberry Slamming\n",
            "Davey Roadkill\n",
            "The Magnolia Anthropy\n",
            "Shemi Hot Man\n",
            "Pain The Dead\n",
            "Rockin' Rocket\n",
            "Stealth Dragon\n",
            "Bashing Bruiser\n",
            "Andi Scarrior Mama\n",
            "Malice N Spine\n",
            "Thunder Beater\n",
            "Countess of Terror\n",
            "Dutch Madness\n",
            "Electric Professor\n",
            "Bad Rita\n",
            "Shelby Twister\n",
            "S\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            " Pain\n",
            "Manip Fil\n",
            "Pandorah\n",
            "Madam Power\n",
            "Tiger Bruise\n",
            "Rita RollHer\n",
            "Agony Fringo\n",
            "Mad-azon DayDudum\n",
            "Georgia Hussy\n",
            "Ivana Go Nuts\n",
            "Bellspick\n",
            "Emun'd Bruiser\n",
            "Em-m Brutal\n",
            "Betty BloX\n",
            "Max Q\n",
            "Tessie Curse\n",
            "Wyld Unition\n",
            "Clumine Dawn\n",
            "Si-Karashnikov\n",
            "Th’emm O. Mouth\n",
            "Yankee Bob\n",
            "Camilla Knowl\n",
            "Chire Cute Locket\n",
            "Victoria Fi\n",
            "\n",
            "cratche Dyde\n",
            "Shred\n",
            "Vicious GI StroyHer\n",
            "Thaireat G’ Spider\n",
            "Jimmy Cowboy\n",
            "Dee’s Cole\n",
            "Hebb Terror\n",
            "Lethal Lady LingMean\n",
            "G. Unga\n",
            "Maximum Malice\n",
            "Cabra Slamma\n",
            "Isabelle Suicide\n",
            "Yuky Von Sleever\n",
            "Massacre Pain\n",
            "Wells Queen\n",
            "Juicy Cleav\n",
            "Ivy Doom\n",
            "Dél-Out Unicorn\n",
            "BENNA NHROLL\n",
            "Angegress CrasHer\n",
            "Paigia Pow\n",
            "Anah Bomb\n",
            "\n",
            "\n",
            " Rousster\n",
            "Little Miss Iron\n",
            "Fe-Rat\n",
            "Khandolina\n",
            "StadtopsyChation\n",
            "Erson Down\n",
            "Jammin' Misdemeanor\n",
            "Traxie Monroll\n",
            "Kat Male\n",
            "Vulurria\n",
            "Lee D Vixe\n",
            "Daisy Missy\n",
            "Wrender B Kitty\n",
            "War Slamwitch\n",
            "Pink Snow Sopran\n",
            "Slappy GoGo\n",
            "Franco Lee\n",
            "Jenny VonRaptor\n",
            "Katipath\n",
            "Krypt Dima\n",
            "Megaheronta\n",
            "Scotty tantrum\n",
            "Hunter Mona\n",
            "Drop D\n",
            "\n",
            "Epoch 11/20\n",
            "961/961 [==============================] - 70s 73ms/step - loss: 1.7230\n",
            "Epoch 12/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.7014\n",
            "Epoch 13/20\n",
            "961/961 [==============================] - 71s 74ms/step - loss: 1.6806\n",
            "Epoch 14/20\n",
            "961/961 [==============================] - 71s 73ms/step - loss: 1.6601\n",
            "Epoch 15/20\n",
            "208/961 [=====>........................] - ETA: 57s - loss: 1.6291"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2G2-IX1pR2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('{}_vocab.json'.format(model_name))\n",
        "files.download('{}_config.json'.format(model_name))\n",
        "files.download('{}_weights.hdf5'.format(model_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cydo9Gro4zcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflowjs as tfjs\n",
        "model_export_dir = \"{}_tfjs\".format(model_name)\n",
        "tfjs.converters.save_keras_model(textgen.model, model_export_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-pMFlw4sYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "prefix = None   # if you want each generated text to start with a given seed text\n",
        "\n",
        "temperature = []\n",
        "for i in range(4):\n",
        "    temp = round(random.random(), 1)\n",
        "    temperature.append(temp)   \n",
        "\n",
        "if train_cfg['line_delimited']:\n",
        "  n = 1000\n",
        "  max_gen_length = 60 if model_cfg['word_level'] else 300\n",
        "else:\n",
        "  n = 10\n",
        "  max_gen_length = 2000 if model_cfg['word_level'] else 10000\n",
        "  \n",
        "generated_names = textgen.generate(n=n, temperature=temperature, return_as_list=True)[0].split('\\n')\n",
        "\n",
        "new_names = [n for n in generated_names if n not in name_list]\n",
        "print(new_names)\n",
        "\n",
        "timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "gen_file = '{}_gentext_{}.txt'.format(model_name, timestring)\n",
        "\n",
        "with open(gen_file, 'w') as f:\n",
        "  f.writelines(\"%s\\n\" % n for n in new_names)\n",
        "\n",
        "\n",
        "# textgen.generate_to_file(gen_file,\n",
        "#                          temperature=temperature,\n",
        "#                          prefix=prefix,\n",
        "#                          n=n,\n",
        "#                          max_gen_length=max_gen_length)\n",
        "\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}